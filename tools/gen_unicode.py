#!/usr/bin/env python3
""" gen_unicode.py - file to generate a portable unicode library 

Check here for information: https://www.unicode.org/reports/tr44/

Usage:

$ ./tools/gen_unicode.py

See the generated header file for the API

@author: Cade Brown <brown.cade@gmail.com>
"""

# std library
import sys
import math
import argparse
import time
import requests

# for cached functions (may help performance)
from functools import lru_cache

# gmpy2: multiprecision
import gmpy2
from gmpy2 import mpfr, const_pi, exp, sqrt, log, log10

# add commandline arguments
parser = argparse.ArgumentParser(formatter_class=lambda prog: argparse.HelpFormatter(prog, max_help_position=40))

parser.add_argument('--prefix', help='Prefix to the C style functions to generate (include the "_"!)', default="ks_")
parser.add_argument('--decl-prefix', help='Prefix to the C style functions to generate (include the "_"!)', default="KS_API")
parser.add_argument('--output', help='Output files (C source, C header)', default=["src/ks-unicode.c", "include/ks-unicode.h"], nargs=2)

args = parser.parse_args()


# variables
prefix = args.prefix
uprefix = args.prefix.upper()

# output files
out_c = args.output[0]
out_h = args.output[1]

# root of the unicode folder
UCD_ROOT = "https://www.unicode.org/Public/13.0.0"

# get source code
UnicodeData_src = requests.get(f"{UCD_ROOT}/ucd/UnicodeData.txt").text

# C source files
unicode_h = ""
unicode_c = ""

# timestamp
times = time.time()


# print header info
unicode_h += f"""/* {out_h} - unicode library header
 * NOTE: Do NOT modify this file! It was generated by `gen_unicode.py`
 *
 *
 * @author: Cade Brown <brown.cade@gmail.com>
 */

#pragma once
#ifndef {uprefix}UNICODE_H__
#define {uprefix}UNICODE_H__


// standard includes
#include <string.h>
#include <stdbool.h>

// integral type representing a single unicode code point
typedef int {prefix}unich;

// indicates there was an error
#define {uprefix}UNICH_ERR (({prefix}unich)(-1))


enum {prefix}unicat {{
    {prefix}unicat_Lu = 1,
    {prefix}unicat_Ll = 2,
    {prefix}unicat_Lt = 3,
    {prefix}unicat_LC = 4,
    {prefix}unicat_Lm = 5,
    {prefix}unicat_Lo = 6,
    {prefix}unicat_L  = 7,
    {prefix}unicat_Mn = 8,
    {prefix}unicat_Mc = 9,
    {prefix}unicat_Me = 10,
    {prefix}unicat_M  = 11,
    {prefix}unicat_Nd = 12,
    {prefix}unicat_Nl = 13,
    {prefix}unicat_No = 14,
    {prefix}unicat_N  = 15,
    {prefix}unicat_Pc = 16,
    {prefix}unicat_Pd = 17,
    {prefix}unicat_Ps = 18,
    {prefix}unicat_Pe = 19,
    {prefix}unicat_Pi = 20,
    {prefix}unicat_Pf = 21,
    {prefix}unicat_Po = 22,
    {prefix}unicat_P  = 23,
    {prefix}unicat_Sm = 24,
    {prefix}unicat_Sc = 25,
    {prefix}unicat_Sk = 26,
    {prefix}unicat_So = 27,
    {prefix}unicat_S  = 28,
    {prefix}unicat_Zs = 29,
    {prefix}unicat_Zl = 30,
    {prefix}unicat_Zp = 31,
    {prefix}unicat_Z  = 32,
    {prefix}unicat_Cc = 33,
    {prefix}unicat_Cf = 34,
    {prefix}unicat_Cs = 35,
    {prefix}unicat_Co = 36,
    {prefix}unicat_Cn = 37,
    {prefix}unicat_C  = 38,
}};


// information about a given codepoint
typedef struct {{

    // (0) the codepoint index of the structure, i.e. the UTF32 value
    {prefix}unich cp;

    // (1) a human readable name, may or may not be in `<>`
    const char* name;

    // (2) General category, check `{prefix}unicat_*` values
    int cat_gen;

    // (3) Canonical combining classes
    int cat_com;

    // (4) Bidirectional category
    int cat_bidi;

    // (5) Character Decomposition Mapping
    int decomp_map;

    // (12, 13, 14) cases for upper, lower, and title case
    {prefix}unich case_upper, case_lower, case_title;

}} {prefix}unich_info;


/* Character Lookup */

// return information about a given codepoint, or NULL if no such codepoint exists
const {prefix}unich_info* {prefix}uni_get_info({prefix}unich chr);


// attempt to look up a codepoint with Unicode name `name` (has length `len`, not counting NUL-terminator), returning
//   either the codepoint, or a negative value to indicate it was not found
// NOTE: `name` should be upper case, include numbers, and match exactly the name found in the Unicode standard
// NOTE: if `len<0`, it is computed via `strlen(name)`
// EXAMPLES:
// uni_lookup("LATIN SMALL LETTER A", -1) == 'a';
{prefix}unich {prefix}uni_lookup(const char* name, int len);


/* Specific Checks (i.e. replacing 'ctype' library) */

// Return whether or not a given unicode character is alpha, i.e. part of the alphabet
bool {prefix}uni_isalpha({prefix}unich chr);


#endif /*{uprefix}UNICODE_H__ */

"""


unicode_c += f"""/* {out_c} - implementation of the unicode library
 * NOTE: Do NOT modify this file! It was generated by `gen_unicode.py`
 *
 *
 * @author: Cade Brown <brown.cade@gmail.com>
 */

#include "{out_h}"


// none/empty codepoint
#define _CP_NONE (({prefix}unich_info) {{ .cp = {uprefix}UNICH_ERR, .name = NULL }})

// all data from 'UnicodeData.txt'
const static {prefix}unich_info _unich_infos[] = {{

"""

# implementation of my hash function in python, which is djb2 with a mask
# this is to ensure everything is the same accross OSes and stuff
def my_hash(st):
    ret = 5381

    for c in map(ord, st):
        ret = ((((ret << 5) & 0xFFFFFFFF) ^ ret) ^ c) & 0xFFFFFFFF

    return ret


    

# current index into the array
idx = 0
max_name_len = 0

# list of (hash, name, name_len, cp) tuples for generating the hash table
ht_kvs = []



# TODO: are others neccessary?
for line in UnicodeData_src.splitlines():
    # check here: https://www.unicode.org/reports/tr44/#UnicodeData.txt
    fields = line.split(';')
    assert len(fields) == 15 and "not enough fields!"


    def tocase(i, v):
        if v:
            return "0x" + v
        else:
            return "0x" + i


    # convert to code point
    cur_cp = int(fields[0], 16)
    assert cur_cp >= idx

    if idx < cur_cp:
        #unicode_c += "    " + "_CP_NONE, " * (cur_cp - idx)
        idx = cur_cp

    while idx < cur_cp:
        #unicode_c += "    _CP_NONE,"
        idx += 1

    # keep track of maximum length
    if len(fields[1]) > max_name_len:
        max_name_len = len(fields[1])


    ht_kvs += [(my_hash(fields[1]), fields[1], len(fields[1]), cur_cp)]

    # generate
    unicode_c += f"    ({prefix}unich_info) {{ .cp = 0x{fields[0]}, .name = \"{fields[1]}\", .cat_gen = ks_unicat_{fields[2]}, .cat_com = ks_unicat_{fields[2]}, .cat_bidi = {fields[3]}, .case_upper = {tocase(fields[0], fields[12])}, .case_lower = {tocase(fields[0], fields[13])}, .case_title = {tocase(fields[0], fields[14])},    }},\n"
    idx += 1

# what load factor should the dictionary have?
load_fac = 0.3

ht_N = int(len(ht_kvs) // load_fac)

unicode_c += f"""
}};


// size of hash table (in elements)
static const int {prefix}ht_N = {ht_N};

// hash table values
static const struct {{

    // hash(name)
    unsigned long long hash;

    // the name
    const char* name;

    // the codepoint it maps to
    {prefix}unich cp;

}} {prefix}ht_V[] = {{

"""


# we need to populate the hash table
ht_V = [None] * ht_N

for hte in ht_kvs:

    # iterate through buckets
    bucket = hte[0] % ht_N

    # original bucket
    o_bucket = bucket

    first = True
    while first or bucket != o_bucket:
        first = False

        if ht_V[bucket] == None:
            # found an empty one, put the value here
            ht_V[bucket] = hte
            break

        # linear probing to the next one
        bucket = (bucket + 1) % ht_N


for V in ht_V:
    # add new hash table entry to C
    if V == None:
        unicode_c += f"    {{ .hash = 0, .name = NULL }},\n"
    else:
        unicode_c += f"    {{ .hash = {V[0]}ULL, .name = \"{V[1]}\", .cp = {hex(V[-1])} }},\n"



unicode_c += f"""
}};



// get unicode information
const {prefix}unich_info* {prefix}uni_get_info({prefix}unich chr) {{
    // binary search
    // left & right notes
    ks_unich l = 0, r = sizeof(_unich_infos) / sizeof(*_unich_infos); 

    while (l <= r) {{
        ks_unich mid = (l + r) / 2;
        if (_unich_infos[mid].cp < chr) {{
            l = mid + 1;
        }} else if (_unich_infos[mid].cp > chr) {{
            r = mid - 1;
        }} else {{
            return &_unich_infos[mid];
        }}

    }}

    // unsuccessful
    return NULL;
    // old way: O(N), but huge memory requirement
    //return &_unich_infos[chr];
}}


// implementation of the masked djb2 hash function (to allow python compatibility)
static unsigned long long _my_hash(const char* st, int len) {{
    unsigned long long ret = 5381;

    int i;
    for (i = 0; i < len; ++i) {{
        ret = ((((ret << 5) & 0xFFFFFFFFULL) ^ ret) ^ st[i]) & 0xFFFFFFFFULL;
    }}

    return ret;
}}

// attempt to look up a codepoint with Unicode name `name` (has length `len`, not counting NUL-terminator), returning
//   either the codepoint, or a negative value to indicate it was not found
// NOTE: `name` should be upper case, include numbers, and match exactly the name found in the Unicode standard
// NOTE: if `len<0`, it is computed via `strlen(name)`
// EXAMPLES:
// uni_lookup("LATIN SMALL LETTER A", -1) == 'a';
{prefix}unich {prefix}uni_lookup(const char* name, int len) {{
    if (len < 0) len = strlen(name);

    // maximum length (in bytes) calculated 
    static const int max_len = {max_name_len};

    // can't be possible
    if (len > max_len) return {uprefix}UNICH_ERR;

    // compute hash
    unsigned long long hash = _my_hash(name, len);

    // iterate through buckets
    unsigned long long bucket = hash % {prefix}ht_N;

    // original bucket
    unsigned long long o_bucket = bucket;

    // probe for the right bucket
    do {{

        if ({prefix}ht_V[bucket].name == NULL) {{
            // nothing found, it must not be valid
            return {uprefix}UNICH_ERR;
        }} else if (hash == {prefix}ht_V[bucket].hash && strncmp(name, {prefix}ht_V[bucket].name, len) == 0) {{
            // found it
            return {prefix}ht_V[bucket].cp;
        }}

        // linear probe for it
        bucket = (bucket + 1) % {prefix}ht_N;
    }} while (bucket != o_bucket);

    // not found
    return {uprefix}UNICH_ERR;
}}


/* Misc. Text functions */

// whether it is a letter
bool {prefix}uni_isalpha({prefix}unich chr) {{
    const {prefix}unich_info* _info = {prefix}uni_get_info(chr);
    if (!_info) return false;

    int cat = _info->cat_gen;
    return cat >= {prefix}unicat_Lu && cat <= {prefix}unicat_L;
}}


"""



# TODO: include encoding/decoding?


unicode_c += f"""


"""


# write files
with open(out_h, 'w') as fp:
    fp.write(unicode_h)

with open(out_c, 'w') as fp:
    fp.write(unicode_c)


